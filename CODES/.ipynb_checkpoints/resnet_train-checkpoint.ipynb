{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993b76b5-4b4d-4201-be86-3a33ad3f196c",
   "metadata": {},
   "source": [
    "# Prediction of Crime rate of 4 different crime types from Street View Images\n",
    "### Resnet18 model trained on places365 dataset is a building block for the model\n",
    "### Steps involved in building and training the model\n",
    "* Converting the ResNet18 model from pytorch to keras framework\n",
    "* Building the model\n",
    "* Creating custom data generator\n",
    "* Training\n",
    "* Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a656e6-f08d-4618-9a9f-0c3fe96f3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "# from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "from pytorch2keras import converter\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torchvision.models as tvmodels\n",
    "from collections import OrderedDict \n",
    "import onnx\n",
    "# from onnx_tf.backend import prepare\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import ResNet50,VGG16\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterator, List, Union, Tuple\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42b8c5-cb6d-4152-9b2b-0acd77ec7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a GPU, if GPU is available, output shows GPU:0\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please ensure you have installed TensorFlow correctly')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77444a6f-51d0-42dc-830b-f4c10d9f2b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory to root directory of the project \n",
    "os.chdir(\"/home/praneeth/THESIS/\")\n",
    "base_path = os.getcwd()\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e22e75-7f26-4e76-bb11-738419ae6cb2",
   "metadata": {},
   "source": [
    "## The Resnet18 model with places365 weights is converted from pytorch to keras framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd806c1d-a8f2-4d92-a509-e842f5b1f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A model is created in pytorch and places365 weights are loaded.\n",
    "Then the pytorch model is converted to keras model and used further.\n",
    "The model is already converted and the weights are saved. No need to perform this step again.\n",
    "\"\"\"\n",
    "resnet_torch = tvmodels.resnet18(pretrained=True)\n",
    "num_ftrs = resnet_torch.fc.in_features\n",
    "resnet_torch.fc = nn.Linear(num_ftrs, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f64ac-3863-4b42-8c7c-d75143b563fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_weights = torch.load('WEIGHTS/resnet18_places365.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb4b5b-ee03-4597-a5ef-79aa41092fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new weights file from the existing places365 as the layer names are different in the existing model.\n",
    "new_state_dict = OrderedDict()\n",
    "for key in torch_weights['state_dict'].keys():\n",
    "    old_key = key.split('.')[1:]\n",
    "    new_key = \".\".join(old_key)\n",
    "    new_state_dict[new_key] = torch_weights['state_dict'][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3270736-83a7-4fbb-a953-3998ca9b6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_weights['state_dict'] = new_state_dict\n",
    "resnet_torch.load_state_dict(torch_weights['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da993a54-b6e5-4282-8901-4089e48833ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the pytorch model to keras model\n",
    "input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n",
    "input_var = Variable(torch.FloatTensor(input_np))\n",
    "resnet_keras = converter.pytorch_to_keras(resnet_torch, input_var, [(3, 224, 224,)],change_ordering=True, verbose=True) \n",
    "resnet_keras.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4297b2-5489-43b1-8a68-ef50db41ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted model\n",
    "resnet_keras.save('WEIGHTS/resnet18_places365.h5') # this will be used in building the 4-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50547083-089a-44c5-b3e6-8461d0c7bd69",
   "metadata": {},
   "source": [
    "## The new model to take multiple inputs & give multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bd53f-0eaf-4542-85f7-6f56f6a53b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name: str):\n",
    "    \"\"\"Accepts the model name as a string and returns multiple callbacks for training the keras model.\n",
    "\n",
    "    Parameters:\n",
    "        model_name : str\n",
    "\n",
    "    Returns\n",
    "        [TensorBoard, EarlyStopping, ModelCheckpoint, LearningRateScheduler] : list\n",
    "        A list of multiple keras callbacks.\n",
    "    \"\"\"\n",
    "    logdir = (\n",
    "        \"logs/scalars/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    )  # create a folder for each model.\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    # use tensorboard --logdir logs/scalars in your command line to startup tensorboard with the correct logs\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=5, # amount of epochs  with improvements worse than 1% until the model stops\n",
    "        verbose=2,\n",
    "        mode=\"min\",\n",
    "        restore_best_weights=True,  # restore the best model with the lowest validation error\n",
    "    )\n",
    "    \n",
    "    weight_path = '/data/private/THESIS/WEIGHTS'  # Set the path of the weights folder\n",
    "    checkpoint_path = os.path.join(weight_path,model_name+'.hdf5')\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath = checkpoint_path,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,  \n",
    "        save_weights_only=True,\n",
    "        mode=\"min\",\n",
    "        save_freq=\"epoch\",  \n",
    "    ) \n",
    "    \n",
    "    def learning_rate_scheduler(epoch,lr):\n",
    "        \"\"\"\n",
    "        Accepts epoch number, learning rate and returns the modified learning rate. Changes for every 5 epochs\n",
    "        \"\"\"\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            lr = lr*0.1\n",
    "        return lr\n",
    "    \n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler,verbose=1)\n",
    "    \n",
    "    return [tensorboard_callback,early_stopping_callback, model_checkpoint_callback,lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f1ab0-2fe3-442f-9056-a22de565e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet():\n",
    "    \"\"\"\n",
    "    Creates resnet model with places365 weights which creates 4 different channels for 4 street view images.\n",
    "    Modifies the existing model and creates a model to take 5 inputs and 4 inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_cnn(suffix=\"\"):\n",
    "        \"\"\"\n",
    "        Creates individual configured model with a suffix for each model.\n",
    "        \"\"\"\n",
    "        \n",
    "        places_model = tf.keras.models.load_model('WEIGHTS/resnet18_places365.h5') # laoding model with places365 weights.\n",
    "        base_model = tf.keras.Model(places_model.input,places_model.layers[-2].output) # removing the top layer of the loaded model.\n",
    "        \n",
    "        for i in range(len(base_model.layers)):\n",
    "            if i<35:\n",
    "                base_model.layers[i].trainable = False # freezes half of the weights in the created model.\n",
    "            base_model.layers[i]._name = base_model.layers[i].name+str(suffix)\n",
    "        return base_model.output,base_model.input\n",
    "    \n",
    "    # creating 4 individual models until top layer.\n",
    "    output_0,input_0 = create_cnn('_0')\n",
    "    output_1,input_1 = create_cnn('_1')\n",
    "    output_2,input_2 = create_cnn('_2')\n",
    "    output_3,input_3 = create_cnn('_3')\n",
    "    numerical_input = layers.Input(shape=(1,))\n",
    "    \n",
    "    merge_outputs = layers.concatenate([output_0,output_1,output_2,output_3, numerical_input]) # merging inputs of models and numerical input.\n",
    "    top_dropout_rate = 0.5\n",
    "    y = layers.Dropout(top_dropout_rate,name=\"top_dropout_0\")(merge_outputs)\n",
    "    y = layers.Dense(1,activation='linear',name='pred')(y)\n",
    "    final_model  = tf.keras.Model(inputs=[input_0,input_1,input_2,input_3, numerical_input],outputs=y) # final model with 5 inputs and 4 outputs.\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f42e5-e70e-44fe-a6fc-26cf58550333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model by the above defined function.\n",
    "new_model = create_resnet()\n",
    "new_model.summary() # prints the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267ae5d-5225-456f-b5a1-9cfd7c67bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model created\n",
    "MODEL_CALLBACKS = get_callbacks(\"resnet18_v1.0\") # give a name to save the model and create a log\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "new_model.compile(optimizer=OPTIMIZER,loss=\"mse\",metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc1b1d-a667-48ac-9392-ff100b92f38b",
   "metadata": {},
   "source": [
    "## Custom data generator to prepare input/output data for the model built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150827a0-5b05-4f0c-b53d-42344738b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \"\"\" A custom data generator class to iterate through the dataset and prepare the inputs and outputs for the model.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col,\n",
    "                 folder_path,\n",
    "                 batch_size,\n",
    "                 input_size=(224, 224, 3),\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.folder_path = folder_path\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle        \n",
    "        self.n = len(self.df)\n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, filename,suffix, target_size,folder_path): \n",
    "        image_path = os.path.join(folder_path,str(filename)+suffix+\".jpg\")\n",
    "        image = tf.keras.preprocessing.image.load_img(image_path)\n",
    "        image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        image_arr = tf.image.central_crop(image_arr,0.50)\n",
    "        image_arr = tf.image.resize(image_arr,(target_size[0], target_size[1])).numpy()\n",
    "        return image_arr/255.\n",
    "    \n",
    "    \n",
    "    def __get_numeric_input(self,den):\n",
    "        return np.array([den])\n",
    "    \n",
    "    def __get_output(self, col1,col2,col3,col4):\n",
    "        output_list = [col1,col2,col3,col4]\n",
    "        return output_list\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "        \"\"\" Generates data containing batch_size samples and returns tensors to be input to the model. \"\"\"\n",
    "        \n",
    "        # input data\n",
    "        filename_batch = batches[self.X_col[0]]\n",
    "        den_batch = batches[self.X_col[1]]\n",
    "        \n",
    "        # output data\n",
    "        burglary_batch = batches[self.y_col[0]]\n",
    "        robbery_batch = batches[self.y_col[1]]\n",
    "        v_crimes_batch = batches[self.y_col[2]]\n",
    "        o_crimes_batch = batches[self.y_col[3]]   \n",
    "               \n",
    "        # preparing input and output data\n",
    "        X_batch_0 = np.asarray([self.__get_input(x,\"_0\", self.input_size,self.folder_path) for x in filename_batch]).astype(np.float32)\n",
    "        X_batch_1 = np.asarray([self.__get_input(x,\"_1\", self.input_size,self.folder_path) for x in filename_batch]).astype(np.float32)\n",
    "        X_batch_2 = np.asarray([self.__get_input(x,\"_2\", self.input_size,self.folder_path) for x in filename_batch]).astype(np.float32)\n",
    "        X_batch_3 = np.asarray([self.__get_input(x,\"_3\", self.input_size,self.folder_path) for x in filename_batch]).astype(np.float32)\n",
    "        X_num = np.asarray([self.__get_numeric_input(x) for x in den_batch]).astype(np.float32)\n",
    "\n",
    "        y_batch = np.asarray([self.__get_output(x,y,z,w) for x,y,z,w in zip(burglary_batch,robbery_batch,v_crimes_batch,o_crimes_batch)],dtype=object).astype(np.float32)\n",
    "\n",
    "        return [X_batch_0,X_batch_1,X_batch_2,X_batch_3,X_num],y_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= self.batch_size:\n",
    "            self.n = 0\n",
    "        result = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return result     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d4fbb-91b1-47ae-a3f9-3d05ccec12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This step is implemented to split the dataset into training, testing and validaiton sets.\n",
    "This step is already implemented and the separated files have been saved locally. No need to perform this step.\n",
    "These files are further used for data input.\n",
    "'''\n",
    "image_path = base_path+\"/DATA/whole_dataset\" # set the dataset path\n",
    "\n",
    "ds = pd.read_csv(\"FILES/whole_ds.csv\") # csv with all points and values\n",
    "\n",
    "# Filtering dataset for records which contains street view images. The other records will be discarded\n",
    "existing_files = natsorted(os.listdir(image_path)) \n",
    "existing_files_unique = list(set([int(z.split(\"_\")[0]) for z in existing_files]))\n",
    "filtered_ds = ds[ds['rand_point'].isin(existing_files_unique)] # this dataframe is used for training, testing and validaiton split\n",
    "\n",
    "# Logarthmic transformation is applied on the data columns for input and output\n",
    "def transform_df(df_ref,col_list):\n",
    "    '''\n",
    "    Takes the dataframe and columns list and returns log-transformed dataframe\n",
    "    '''\n",
    "    df_tr = df_ref.copy()\n",
    "    for i in col_list:\n",
    "        df_tr[i] +=1\n",
    "        df_tr[i] = np.log(df_tr[i])\n",
    "    return df_tr\n",
    "filtered_ds = transform_df(filtered_ds,['burglary','robbery','o_crimes','v_crimes']) # log-transformed dataset\n",
    "\n",
    "# Splitting the dataset to training, validation and test dataset\n",
    "ds_group = filtered_ds.groupby('pointid')\n",
    "test, validate, train = np.split(ds_group.sample(frac=1, random_state=42),[int(.2*len(ds_group)), int(.4*len(ds_group))])\n",
    "train_cells = list(set(train['pointid'].tolist()))\n",
    "train_set = filtered_ds[filtered_ds['pointid'].isin(train_cells)]\n",
    "validate_cells = list(set(validate['pointid'].tolist()))\n",
    "validate_set = filtered_ds[filtered_ds['pointid'].isin(validate_cells)]\n",
    "test_cells = list(set(test['pointid'].tolist()))\n",
    "test_set = filtered_ds[filtered_ds['pointid'].isin(test_cells)]\n",
    "\n",
    "# Saving the split datasets for replciation\n",
    "train_set.to_csv(\"FILES/train_split.csv\", index = False)\n",
    "validate_set.to_csv(\"FILES/validate_split.csv\", index = False)\n",
    "test_set.to_csv(\"FILES/test_split.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9bc8a-2a14-49cb-b42e-0b8177ee63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = base_path+\"/DATA/whole_dataset\" # set the dataset path\n",
    "\n",
    "# Load training, validation and testing files\n",
    "train_split = pd.read_csv(\"FILES/train_split.csv\")\n",
    "train_split = train_split.sample(frac=1) # shuffles the records in the table for training set\n",
    "validation_split = pd.read_csv(\"FILES/validation_split.csv\")\n",
    "test_split = pd.read_csv(\"FILES/test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f93e3-8c34-4009-ac91-ccf284278bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_gen = CustomDataGen(\n",
    "                df = train_split,\n",
    "                X_col = [\"rand_point\",\"POPDEN\"],\n",
    "                y_col = [\"burglary\",\"robbery\",\"o_crimes\",\"v_crimes\"],\n",
    "                folder_path = image_path,\n",
    "                batch_size = 8,\n",
    "                input_size=(224, 224, 3),\n",
    "                shuffle=True) # training data input\n",
    "vs_gen = CustomDataGen(\n",
    "                df = validation_split,\n",
    "                X_col = [\"rand_point\",\"POPDEN\"],\n",
    "                y_col = [\"burglary\",\"robbery\",\"o_crimes\",\"v_crimes\"],\n",
    "                folder_path = image_path,\n",
    "                batch_size = 8,\n",
    "                input_size=(224, 224, 3),\n",
    "                shuffle=True) # validation data input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df16d3-ed2a-4589-9a57-296e6f8d037d",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bbda8-7015-4748-9c1e-4fba169517cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model and saving the history to \n",
    "resnet18_history = new_model.fit(\n",
    "        x = tr_gen,\n",
    "        epochs=20,\n",
    "        validation_data=tr_gen,\n",
    "        callbacks=MODEL_CALLBACKS,\n",
    "        verbose=1,\n",
    "        workers=4, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ee0f7-2ac3-4e7c-92c9-e5593c42e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(new_model, to_file=\"resnet18_v1.0.jpg\", show_shapes=True) # plot and save the built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73852089-71d9-47ce-a338-27b2ba436005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training curves for loss and evaluation metrics\n",
    "\n",
    "print(resnet18_history.history.keys())\n",
    "# summarize history for evaluation metrics\n",
    "plt.plot(resnet18_history.history['mae'])\n",
    "plt.plot(resnet18_history.history['val_mae'])\n",
    "plt.title('model mean absolute error')\n",
    "plt.ylabel('mean absolute error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(resnet18_history.history['loss'])\n",
    "plt.plot(resnet18_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34204460-594c-4322-a291-492ab4b6094b",
   "metadata": {},
   "source": [
    "## Predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91af79-1332-4f88-a6ea-86a1b4310fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_gen = CustomDataGen(\n",
    "                df = test_split,\n",
    "                X_col = [\"rand_point\",\"POPDEN\"],\n",
    "                y_col = [\"burglary\",\"robbery\",\"v_crimes\",\"o_crimes\"],\n",
    "                folder_path = image_path,\n",
    "                batch_size = 1,\n",
    "                input_size=(224, 224, 3),\n",
    "                shuffle=False) # dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e827b6b-fb78-4aff-8466-d48acd31d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_outputs = new_model.predict(pr_gen) # outputs for prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c7ae3-fdbc-4492-a7fb-6771b350d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from predcitions and merging to the actual values\n",
    "pred_df = pd.DataFrame(pred_outputs,columns=[\"burglary_p\",\"robbery_p\",\"v_crimes_p\",\"o_crimes_p\"])\n",
    "pr_filename = train_split['rand_point'].tolist()\n",
    "pred_df['rand_point'] = pr_filename\n",
    "final_df = pd.merge(train_split, pred_df, on = 'rand_point')\n",
    "\n",
    "# Saving the predictions to a file\n",
    "final_df.to_csv(\"FILES/resnet18_51_v6.0_scaled_ds_tr.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
